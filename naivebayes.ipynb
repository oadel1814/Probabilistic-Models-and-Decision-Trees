{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"adult.csv\")\n",
    "\n",
    "\n",
    "# choosing the discerete features\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"education\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"native-country\",\n",
    "]\n",
    "\n",
    "target_col = \"income\"     \n",
    "\n",
    "df = df[categorical_features + [target_col]]\n",
    "\n",
    "\n",
    "# stripping whitespace from categorical features and target column\n",
    "# making ? a separate category to include it in calculations\n",
    "for col in categorical_features + [target_col]:\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "df = df.fillna(\"?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mappings = {}\n",
    "\n",
    "def encode_column(series):\n",
    "    \"\"\"\n",
    "    Encode a pandas Series of strings into integers and\n",
    "    return the encoded series + mapping dict.\n",
    "    \"\"\"\n",
    "    unique_values = sorted(series.unique())  # sort for consistency\n",
    "    mapping = {value: idx for idx, value in enumerate(unique_values)}\n",
    "    encoded = series.map(mapping)\n",
    "    return encoded, mapping\n",
    "\n",
    "# Encode all categorical features\n",
    "for col in categorical_features:\n",
    "    encoded_col, mapping = encode_column(df[col])\n",
    "    df[col] = encoded_col\n",
    "    feature_mappings[col] = mapping\n",
    "\n",
    "# Encode target (income) to 0/1\n",
    "# Example: '<=50K' -> 0, '>50K' -> 1\n",
    "target_values = sorted(df[target_col].unique())\n",
    "target_mapping = {value: idx for idx, value in enumerate(target_values)}\n",
    "df[target_col] = df[target_col].map(target_mapping)\n",
    "\n",
    "print(\"Target mapping:\", target_mapping)\n",
    "print(\"\\nFeature mappings example (first feature):\")\n",
    "first_feat = categorical_features[0]\n",
    "print(first_feat, \":\", feature_mappings[first_feat])\n",
    "\n",
    "\n",
    "#  Split into 70% / 15% / 15% (stratified)\n",
    "\n",
    "X = df[categorical_features].values\n",
    "y = df[target_col].values\n",
    "\n",
    "# First: train (70%) vs temp (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Second: split temp into validation (15%) and test (15%)\n",
    "# 0.15 / 0.30 = 0.5  → so we split temp 50/50\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.50,\n",
    "    stratify=y_temp,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# 6) Analyze class distribution\n",
    "def describe_class_distribution(y, name):\n",
    "    unique, counts = pd.Series(y).value_counts().sort_index().index, pd.Series(y).value_counts().sort_index().values\n",
    "    total = len(y)\n",
    "    print(f\"\\nClass distribution in {name}:\")\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        pct = cnt / total * 100\n",
    "        # Map back to original label using inverse mapping\n",
    "        inv_target_mapping = {v: k for k, v in target_mapping.items()}\n",
    "        print(f\"  Class {cls} ({inv_target_mapping[cls]}): {cnt} samples ({pct:.2f}%)\")\n",
    "\n",
    "describe_class_distribution(y_train, \"TRAIN\")\n",
    "describe_class_distribution(y_val, \"VALIDATION\")\n",
    "describe_class_distribution(y_test, \"TEST\")\n",
    "\n",
    "\n",
    "#  Analyze feature–target relationships (on full data or train only)\n",
    "\n",
    "\n",
    "full_df = df.copy()\n",
    "full_df[target_col] = full_df[target_col].map({v: k for k, v in target_mapping.items()})  # back to original labels\n",
    "\n",
    "print(\"\\nFeature–target relationships (examples):\")\n",
    "\n",
    "for col in [\"education\", \"marital-status\", \"workclass\"]:\n",
    "    print(f\"\\n=== {col} vs income ===\")\n",
    "    # Map ints back to categories for readability\n",
    "    inv_map = {v: k for k, v in feature_mappings[col].items()}\n",
    "    temp = full_df[[col, target_col]].copy()\n",
    "    temp[col] = temp[col].map(inv_map)\n",
    "    \n",
    "    # Crosstab with row-normalized percentages\n",
    "    ct = pd.crosstab(temp[col], temp[target_col], normalize='index') * 100\n",
    "    print(ct.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bae9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B2 \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def train_naive_bayes_discrete(X_train, y_train, n_classes, alpha=1.0):\n",
    " \n",
    "    N, d = X_train.shape\n",
    "\n",
    "    #Class counts and priors P(C_k)\n",
    "    class_counts = np.bincount(y_train, minlength=n_classes)  # shape (n_classes,)\n",
    "    class_priors = (class_counts + alpha) / (N + alpha * n_classes)  # shape (n_classes,)\n",
    "\n",
    "    #  For each feature j, we need number of possible values V_j\n",
    "    n_values_per_feature = []\n",
    "    for j in range(d):\n",
    "        n_values_per_feature.append(int(X_train[:, j].max()) + 1)  # assume values start at 0\n",
    "\n",
    "    #  Feature likelihood counts: count[x_j = v, class = k]\n",
    "    # We'll store as a list of arrays: feature_counts[j].shape = (V_j, n_classes)\n",
    "    feature_counts = []\n",
    "    for j in range(d):\n",
    "        V_j = n_values_per_feature[j]\n",
    "        counts_j = np.zeros((V_j, n_classes), dtype=np.float64)\n",
    "        feature_counts.append(counts_j)\n",
    "\n",
    "    # Fill counts\n",
    "    for i in range(N):\n",
    "        x = X_train[i]\n",
    "        c = y_train[i]\n",
    "        for j in range(d):\n",
    "            v = x[j]\n",
    "            feature_counts[j][v, c] += 1.0\n",
    "\n",
    "    # 4) Convert counts to probabilities with Laplace smoothing:\n",
    "    # P(x_j = v | C_k) = (count + alpha) / (class_counts[k] + alpha * V_j)\n",
    "    feature_likelihoods = []\n",
    "    for j in range(d):\n",
    "        V_j = n_values_per_feature[j]\n",
    "        counts_j = feature_counts[j]  # (V_j, n_classes)\n",
    "        probs_j = np.zeros_like(counts_j)\n",
    "        for k in range(n_classes):\n",
    "            probs_j[:, k] = (counts_j[:, k] + alpha) / (class_counts[k] + alpha * V_j)\n",
    "        feature_likelihoods.append(probs_j)\n",
    "\n",
    "    model = {\n",
    "        \"class_priors\": class_priors,                \n",
    "        \"feature_likelihoods\": feature_likelihoods,  \n",
    "        \"n_values_per_feature\": n_values_per_feature\n",
    "    }\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_proba(model, X):\n",
    "    \n",
    "    class_priors = model[\"class_priors\"]\n",
    "    feature_likelihoods = model[\"feature_likelihoods\"]\n",
    "\n",
    "    X = np.asarray(X, dtype=np.int64)\n",
    "    N, d = X.shape\n",
    "    n_classes = class_priors.shape[0]\n",
    "\n",
    "    log_priors = np.log(class_priors)  # (n_classes,)\n",
    "    log_probs = np.zeros((N, n_classes), dtype=np.float64)\n",
    "\n",
    "    for n in range(N):\n",
    "        x = X[n]\n",
    "        log_p = log_priors.copy()\n",
    "        for j in range(d):\n",
    "            v = x[j]\n",
    "            probs_j = feature_likelihoods[j]  # shape (V_j, n_classes)\n",
    "            log_p += np.log(probs_j[v, :])\n",
    "        log_probs[n, :] = log_p\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "def predict(model, X):\n",
    "    log_probs = predict_log_proba(model, X)\n",
    "    return np.argmax(log_probs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "val_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = train_naive_bayes_discrete(X_train, y_train, n_classes, alpha=alpha)\n",
    "    y_val_pred = predict(model, X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_results.append((alpha, val_acc))\n",
    "    print(f\"alpha = {alpha}, validation accuracy = {val_acc:.4f}\")\n",
    "\n",
    "best_alpha, best_val_acc = max(val_results, key=lambda t: t[1])\n",
    "print(\"\\nBest alpha:\", best_alpha, \"with validation accuracy:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6120c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Merge train + val\n",
    "X_train_full = np.vstack([X_train, X_val])\n",
    "y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "# Train final model with best alpha\n",
    "best_model = train_naive_bayes_discrete(X_train_full, y_train_full, n_classes, alpha=best_alpha)\n",
    "y_test_pred = predict(best_model, X_test)\n",
    "\n",
    "def evaluate_all_metrics(y_true, y_pred, name=\"\"):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Per-class precision/recall/F1\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None\n",
    "    )\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):\n",
    "        print(f\"Class {i}: precision={p:.4f}, recall={r:.4f}, f1={f:.4f}, support={s}\")\n",
    "    \n",
    "    # Macro-averaged metrics\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"macro\"\n",
    "    )\n",
    "    print(\"\\nMacro-averaged:\")\n",
    "    print(f\"Precision_macro={precision_macro:.4f}\")\n",
    "    print(f\"Recall_macro   ={recall_macro:.4f}\")\n",
    "    print(f\"F1_macro       ={f1_macro:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nConfusion matrix (rows=true, cols=pred):\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Optional pretty text report\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }\n",
    "\n",
    "metrics_test = evaluate_all_metrics(y_test, y_test_pred, name=\"Our Naive Bayes (Test)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59401e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "sk_model = MultinomialNB(alpha=best_alpha)\n",
    "sk_model.fit(X_train_full, y_train_full)\n",
    "y_test_pred_sk = sk_model.predict(X_test)\n",
    "\n",
    "metrics_test_sk = evaluate_all_metrics(y_test, y_test_pred_sk, name=\"sklearn MultinomialNB (Test)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cfae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probs_to_probs(log_probs):\n",
    "    # log_probs shape: (N, n_classes)\n",
    "    max_log = np.max(log_probs, axis=1, keepdims=True)\n",
    "    stabilized = log_probs - max_log\n",
    "    exp_vals = np.exp(stabilized)\n",
    "    probs = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "    return probs\n",
    "\n",
    "log_probs_test = predict_log_proba(best_model, X_test)\n",
    "probs_test = log_probs_to_probs(log_probs_test)  # shape (N, n_classes)\n",
    "\n",
    "# probability of class 1 (e.g., >50K) for each sample\n",
    "p_class1 = probs_test[:, 1]\n",
    "\n",
    "print(\"\\nProbability analysis for class 1 (>50K):\")\n",
    "print(\"Mean probability:\", p_class1.mean())\n",
    "print(\"Min probability :\", p_class1.min())\n",
    "print(\"Max probability :\", p_class1.max())\n",
    "\n",
    "# simple bins: how confident is the model?\n",
    "bins = [0.0, 0.5, 0.7, 0.9, 1.0]\n",
    "counts = np.histogram(p_class1, bins=bins)[0]\n",
    "total = len(p_class1)\n",
    "for (b1, b2), c in zip(zip(bins[:-1], bins[1:]), counts):\n",
    "    print(f\"P(class=1) in [{b1:.1f}, {b2:.1f}): {c} samples ({c/total*100:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
